{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (layer1): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (layer2): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the neural network class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialize the parent class\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # First fully connected layer: input size 2, output size 10\n",
    "        self.layer1 = nn.Linear(in_features=2, out_features=10)\n",
    "        # ReLU activation function to be used after layer1\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second fully connected layer: input size 10, output size 1\n",
    "        self.layer2 = nn.Linear(in_features=10, out_features=1)\n",
    "        # Sigmoid activation function to be used after layer2\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer1 (input: 2, output: 10)\n",
    "        x = self.layer1(x)\n",
    "        # Apply ReLU activation function (output: 10)\n",
    "        x = self.relu(x)\n",
    "        # Apply layer2 (input: 10, output: 1)\n",
    "        x = self.layer2(x)\n",
    "        # Apply Sigmoid activation function (output: 1)\n",
    "        x = self.sigmoid(x)\n",
    "        # Return the output\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Print the model's architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (layer1): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (layer2): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the neural network class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialize the parent class\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # First fully connected layer: input size 4, output size 10\n",
    "        self.layer1 = nn.Linear(in_features=4, out_features=10)\n",
    "        # ReLU activation function to be used after layer1\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second fully connected layer: input size 10, output size 2\n",
    "        self.layer2 = nn.Linear(in_features=10, out_features=2)  \n",
    "        # Sigmoid activation function to be used after layer2\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer1 (input: 4, output: 10)\n",
    "        x = self.layer1(x)\n",
    "        # Apply ReLU activation function (output: 10)\n",
    "        x = self.relu(x)\n",
    "        # Apply layer2 (input: 10, output: 2)\n",
    "        x = self.layer2(x)\n",
    "        # Apply Sigmoid activation function (output: 1)\n",
    "        x = self.sigmoid(x)\n",
    "        # Return the output\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Print the model's architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyOwnNN(\n",
      "  (layer1): Linear(in_features=5, out_features=10, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (layer2): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (sigmoid1): Sigmoid()\n",
      "  (layer3): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (sigmoid2): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the neural network class\n",
    "class MyOwnNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialize the parent class\n",
    "        super(MyOwnNN, self).__init__()\n",
    "        # First fully connected layer: input size 5, output size 10\n",
    "        self.layer1 = nn.Linear(in_features=5, out_features=10)\n",
    "        # ReLU activation function to be used after layer1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Second fully connected layer: input size 10, output size 20\n",
    "        self.layer2 = nn.Linear(in_features=10, out_features=20)\n",
    "        # Sigmoid activation function to be used after layer2\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        # Incorrectly created third fully connected layer\n",
    "        self.layer3 = nn.Linear(in_features=20, out_features=10)  \n",
    "        # Sigmoid activation function to be used after layer3\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer1 (input: 5, output: 10)\n",
    "        x = self.layer1(x)\n",
    "        # Apply ReLU activation function\n",
    "        x = self.relu1(x)\n",
    "        # Apply layer2 (input: 10, output: 20)\n",
    "        x = self.layer2(x)\n",
    "        # Apply Sigmoid activation function\n",
    "        x = self.sigmoid1(x)\n",
    "        # Apply layer3 (input: 2, output: 10) â€” Error here due to incorrect input size\n",
    "        x = self.layer3(x)\n",
    "        # Apply Sigmoid activation function\n",
    "        x = self.sigmoid2(x)\n",
    "        # Return the output\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyOwnNN()\n",
    "\n",
    "# Print the model's architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (layer1): Linear(in_features=3, out_features=16, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (layer2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (layer3): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# TODO: Define the neural network class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize the parent class\n",
    "        super (SimpleNN,self).__init__()\n",
    "        # TODO: Define first fully connected layer: input size 3, output size 16\n",
    "        self.layer1 = nn.Linear(in_features=3,out_features=16)\n",
    "        # TODO: Define ReLU activation function (for layer1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # TODO: Define second fully connected layer: input size 16, output size 8\n",
    "        self.layer2 = nn.Linear(in_features=16,out_features=8)\n",
    "        # TODO: Define ReLU activation function (for layer2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # TODO: Define third fully connected layer: input size 8, output size 1\n",
    "        self.layer3 = nn.Linear(in_features=8,out_features=1)\n",
    "        # TODO: Define Sigmoid activation function (for layer3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer1 (input: 3, output: 16)\n",
    "        x = self.layer1(x)\n",
    "        # Apply ReLU activation function (output: 16)\n",
    "        x = self.relu1(x)\n",
    "        # Apply layer2 (input: 16, output: 8)\n",
    "        x = self.layer2(x)\n",
    "        # Apply ReLU activation function (output: 8)\n",
    "        x = self.relu2(x)\n",
    "        # Apply layer3 (input: 8, output: 1)\n",
    "        x = self.layer3(x)\n",
    "        # Apply Sigmoid activation function (output: 1)\n",
    "        x = self.sigmoid(x)\n",
    "        # Return the output\n",
    "        return x\n",
    "\n",
    "# TODO: Instantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# TODO: Display model's architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (layer1): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (layer2): Linear(in_features=20, out_features=15, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (layer3): Linear(in_features=15, out_features=5, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# TODO: Define the neural network class\n",
    "class SimpleNN(nn.Module):\n",
    "    # TODO: Define init function\n",
    "    def __init__(self):\n",
    "        super(SimpleNN,self).__init__()\n",
    "        # TODO: Initialize the parent class\n",
    "        # TODO: First fully connected layer (input: 10, output: 20)\n",
    "        self.layer1 = nn.Linear(in_features = 10, out_features = 20) \n",
    "        # TODO: ReLU activation function \n",
    "        self.relu1 = nn.ReLU()\n",
    "        # TODO: Second fully connected layer (input: 20, output: 15)\n",
    "        self.layer2 = nn.Linear(in_features=20,out_features=15)\n",
    "        # TODO: ReLU activation function \n",
    "        self.relu2 = nn.ReLU()\n",
    "        # TODO: Third fully connected layer (input: 15, output: 5)\n",
    "        self.layer3 = nn.Linear(in_features=15,out_features=5)\n",
    "        # TODO: Sigmoid activation function to be used after layer3\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # TODO: Define forward method\n",
    "    def forward(self,x):\n",
    "        # TODO: Apply layer1\n",
    "        x = self.layer1(x)\n",
    "        # TODO: Apply ReLU activation function\n",
    "        x = self.relu1(x)\n",
    "        # TODO: Apply layer2\n",
    "        x = self.layer2(x)\n",
    "        # TODO: Apply ReLU activation function\n",
    "        x = self.relu2(x)\n",
    "        # TODO: Apply layer3\n",
    "        x = self.layer3(x)\n",
    "        # TODO: Apply Sigmoid activation function \n",
    "        x = self.sigmoid(x)\n",
    "        # TODO: Return the output\n",
    "        return x\n",
    "\n",
    "# TODO: Instantiate the model\n",
    "model = SimpleNN()\n",
    "# TODO: Print the model's architecture\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
