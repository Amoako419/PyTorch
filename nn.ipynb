{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (layer1): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (layer2): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the neural network class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialize the parent class\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # First fully connected layer: input size 2, output size 10\n",
    "        self.layer1 = nn.Linear(in_features=2, out_features=10)\n",
    "        # ReLU activation function to be used after layer1\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second fully connected layer: input size 10, output size 1\n",
    "        self.layer2 = nn.Linear(in_features=10, out_features=1)\n",
    "        # Sigmoid activation function to be used after layer2\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer1 (input: 2, output: 10)\n",
    "        x = self.layer1(x)\n",
    "        # Apply ReLU activation function (output: 10)\n",
    "        x = self.relu(x)\n",
    "        # Apply layer2 (input: 10, output: 1)\n",
    "        x = self.layer2(x)\n",
    "        # Apply Sigmoid activation function (output: 1)\n",
    "        x = self.sigmoid(x)\n",
    "        # Return the output\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Print the model's architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (layer1): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (layer2): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the neural network class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialize the parent class\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # First fully connected layer: input size 4, output size 10\n",
    "        self.layer1 = nn.Linear(in_features=4, out_features=10)\n",
    "        # ReLU activation function to be used after layer1\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second fully connected layer: input size 10, output size 2\n",
    "        self.layer2 = nn.Linear(in_features=10, out_features=2)  \n",
    "        # Sigmoid activation function to be used after layer2\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer1 (input: 4, output: 10)\n",
    "        x = self.layer1(x)\n",
    "        # Apply ReLU activation function (output: 10)\n",
    "        x = self.relu(x)\n",
    "        # Apply layer2 (input: 10, output: 2)\n",
    "        x = self.layer2(x)\n",
    "        # Apply Sigmoid activation function (output: 1)\n",
    "        x = self.sigmoid(x)\n",
    "        # Return the output\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Print the model's architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyOwnNN(\n",
      "  (layer1): Linear(in_features=5, out_features=10, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (layer2): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (sigmoid1): Sigmoid()\n",
      "  (layer3): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (sigmoid2): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the neural network class\n",
    "class MyOwnNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialize the parent class\n",
    "        super(MyOwnNN, self).__init__()\n",
    "        # First fully connected layer: input size 5, output size 10\n",
    "        self.layer1 = nn.Linear(in_features=5, out_features=10)\n",
    "        # ReLU activation function to be used after layer1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Second fully connected layer: input size 10, output size 20\n",
    "        self.layer2 = nn.Linear(in_features=10, out_features=20)\n",
    "        # Sigmoid activation function to be used after layer2\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        # Incorrectly created third fully connected layer\n",
    "        self.layer3 = nn.Linear(in_features=20, out_features=10)  \n",
    "        # Sigmoid activation function to be used after layer3\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer1 (input: 5, output: 10)\n",
    "        x = self.layer1(x)\n",
    "        # Apply ReLU activation function\n",
    "        x = self.relu1(x)\n",
    "        # Apply layer2 (input: 10, output: 20)\n",
    "        x = self.layer2(x)\n",
    "        # Apply Sigmoid activation function\n",
    "        x = self.sigmoid1(x)\n",
    "        # Apply layer3 (input: 2, output: 10) â€” Error here due to incorrect input size\n",
    "        x = self.layer3(x)\n",
    "        # Apply Sigmoid activation function\n",
    "        x = self.sigmoid2(x)\n",
    "        # Return the output\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyOwnNN()\n",
    "\n",
    "# Print the model's architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (layer1): Linear(in_features=3, out_features=16, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (layer2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (layer3): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# TODO: Define the neural network class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize the parent class\n",
    "        super (SimpleNN,self).__init__()\n",
    "        # TODO: Define first fully connected layer: input size 3, output size 16\n",
    "        self.layer1 = nn.Linear(in_features=3,out_features=16)\n",
    "        # TODO: Define ReLU activation function (for layer1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # TODO: Define second fully connected layer: input size 16, output size 8\n",
    "        self.layer2 = nn.Linear(in_features=16,out_features=8)\n",
    "        # TODO: Define ReLU activation function (for layer2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # TODO: Define third fully connected layer: input size 8, output size 1\n",
    "        self.layer3 = nn.Linear(in_features=8,out_features=1)\n",
    "        # TODO: Define Sigmoid activation function (for layer3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer1 (input: 3, output: 16)\n",
    "        x = self.layer1(x)\n",
    "        # Apply ReLU activation function (output: 16)\n",
    "        x = self.relu1(x)\n",
    "        # Apply layer2 (input: 16, output: 8)\n",
    "        x = self.layer2(x)\n",
    "        # Apply ReLU activation function (output: 8)\n",
    "        x = self.relu2(x)\n",
    "        # Apply layer3 (input: 8, output: 1)\n",
    "        x = self.layer3(x)\n",
    "        # Apply Sigmoid activation function (output: 1)\n",
    "        x = self.sigmoid(x)\n",
    "        # Return the output\n",
    "        return x\n",
    "\n",
    "# TODO: Instantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# TODO: Display model's architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (layer1): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (layer2): Linear(in_features=20, out_features=15, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (layer3): Linear(in_features=15, out_features=5, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# TODO: Define the neural network class\n",
    "class SimpleNN(nn.Module):\n",
    "    # TODO: Define init function\n",
    "    def __init__(self):\n",
    "        super(SimpleNN,self).__init__()\n",
    "        # TODO: Initialize the parent class\n",
    "        # TODO: First fully connected layer (input: 10, output: 20)\n",
    "        self.layer1 = nn.Linear(in_features = 10, out_features = 20) \n",
    "        # TODO: ReLU activation function \n",
    "        self.relu1 = nn.ReLU()\n",
    "        # TODO: Second fully connected layer (input: 20, output: 15)\n",
    "        self.layer2 = nn.Linear(in_features=20,out_features=15)\n",
    "        # TODO: ReLU activation function \n",
    "        self.relu2 = nn.ReLU()\n",
    "        # TODO: Third fully connected layer (input: 15, output: 5)\n",
    "        self.layer3 = nn.Linear(in_features=15,out_features=5)\n",
    "        # TODO: Sigmoid activation function to be used after layer3\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # TODO: Define forward method\n",
    "    def forward(self,x):\n",
    "        # TODO: Apply layer1\n",
    "        x = self.layer1(x)\n",
    "        # TODO: Apply ReLU activation function\n",
    "        x = self.relu1(x)\n",
    "        # TODO: Apply layer2\n",
    "        x = self.layer2(x)\n",
    "        # TODO: Apply ReLU activation function\n",
    "        x = self.relu2(x)\n",
    "        # TODO: Apply layer3\n",
    "        x = self.layer3(x)\n",
    "        # TODO: Apply Sigmoid activation function \n",
    "        x = self.sigmoid(x)\n",
    "        # TODO: Return the output\n",
    "        return x\n",
    "\n",
    "# TODO: Instantiate the model\n",
    "model = SimpleNN()\n",
    "# TODO: Print the model's architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Model Architecture:\n",
      " Sequential(\n",
      "  (0): Linear(in_features=2, out_features=5, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=5, out_features=1, bias=True)\n",
      "  (3): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Creating a basic sequential model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 5),    # First layer: input size 2, output size 5\n",
    "    nn.ReLU(),           # ReLU activation function\n",
    "    nn.Linear(5, 1),   # Second layer: input size 5, output size 1\n",
    "    nn.Sigmoid()         # Sigmoid activation function\n",
    ")\n",
    "\n",
    "# Print model's architecture\n",
    "print(\"Sequential Model Architecture:\\n\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=7, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=7, out_features=5, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=5, out_features=1, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# TODO: Create your Sequential model here\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3,7),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(7,5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# TODO: Print the model's architecture here\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.4143106937408447\n",
      "Epoch 20, Loss: 0.18726857006549835\n",
      "Epoch 30, Loss: 0.08348482847213745\n",
      "Epoch 40, Loss: 0.04321146011352539\n",
      "Epoch 50, Loss: 0.025807388126850128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Input features [Average Goals Scored, Average Goals Conceded by Opponent]\n",
    "X = torch.tensor([\n",
    "    [3.0, 0.5], [1.0, 1.0], [0.5, 2.0], [2.0, 1.5],\n",
    "    [3.5, 3.0], [2.0, 2.5], [1.5, 1.0], [0.5, 0.5],\n",
    "    [2.5, 0.8], [2.1, 2.0], [1.2, 0.5], [0.7, 1.5]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Target outputs [1 if the team is likely to win, 0 otherwise]\n",
    "y = torch.tensor([[1], [0], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)  \n",
    "\n",
    "# Train the model for 50 epochs\n",
    "for epoch in range(50):  \n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    optimizer.zero_grad()  # Zero the gradients for this iteration\n",
    "\n",
    "    outputs = model(X)  # Forward pass: compute predictions\n",
    "\n",
    "    loss = criterion(outputs, y)  # Compute the loss\n",
    "\n",
    "    loss.backward()  # Backward pass: compute the gradient of the loss\n",
    "\n",
    "    optimizer.step()  # Optimize the model parameters based on the gradients\n",
    "\n",
    "    if (epoch+1) % 10 == 0:  # Print every 10 epochs\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")  # Print epoch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.5780903697013855\n",
      "Epoch 20, Loss: 0.488630086183548\n",
      "Epoch 30, Loss: 0.39655637741088867\n",
      "Epoch 40, Loss: 0.31670716404914856\n",
      "Epoch 50, Loss: 0.254350870847702\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Input features [Average Goals Scored, Average Goals Conceded by Opponent]\n",
    "X = torch.tensor([\n",
    "    [3.0, 0.5], [1.0, 1.0], [0.5, 2.0], [2.0, 1.5],\n",
    "    [3.5, 3.0], [2.0, 2.5], [1.5, 1.0], [0.5, 0.5],\n",
    "    [2.5, 0.8], [2.1, 2.0], [1.2, 0.5], [0.7, 1.5]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Target outputs [1 if the team is likely to win, 0 otherwise]\n",
    "y = torch.tensor([[1], [0], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Original learning rate\n",
    "\n",
    "# Train the model for 50 epochs\n",
    "for epoch in range(50):  \n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    optimizer.zero_grad()  # Zero the gradients for this iteration\n",
    "\n",
    "    outputs = model(X)  # Forward pass: compute predictions\n",
    "\n",
    "    loss = criterion(outputs, y)  # Compute the loss\n",
    "\n",
    "    loss.backward()  # Backward pass: compute the gradient of the loss\n",
    "\n",
    "    optimizer.step()  # Optimize the model parameters based on the gradients\n",
    "\n",
    "    if (epoch+1) % 10 == 0:  # Print every 10 epochs\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")  # Print epoch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output: tensor([[0.6940]])\n",
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Input features [Average Goals Scored, Average Goals Conceded by Opponent]\n",
    "X = torch.tensor([\n",
    "    [3.0, 0.5], [1.0, 1.0], [0.5, 2.0], [2.0, 1.5],\n",
    "    [3.5, 3.0], [2.0, 2.5], [1.5, 1.0], [0.5, 0.5],\n",
    "    [2.5, 0.8], [2.1, 2.0], [1.2, 0.5], [0.7, 1.5]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Target outputs [1 if the team is likely to win, 0 otherwise]\n",
    "y = torch.tensor([[1], [0], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  \n",
    "\n",
    "# Train the model for 50 epochs\n",
    "for epoch in range(50):  \n",
    "    model.train()  # Set the model to training mode\n",
    "    optimizer.zero_grad()  # Zero the gradients for iteration\n",
    "    outputs = model(X)  # Compute predictions\n",
    "    loss = criterion(outputs, y)  # Compute the loss\n",
    "    loss.backward()  # Compute the gradient of the loss\n",
    "    optimizer.step()  # Optimize the model parameters\n",
    "\n",
    "# Create a new input tensor\n",
    "new_input = torch.tensor([[3.0, 2.5]], dtype=torch.float32)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    # Make a prediction for the new input\n",
    "    prediction = model(new_input)\n",
    "\n",
    "# Print the raw output from the model\n",
    "print(\"Raw output:\", prediction)\n",
    "\n",
    "# Convert the probability to a binary class label\n",
    "print(\"Prediction:\", (prediction > 0.5).int().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
