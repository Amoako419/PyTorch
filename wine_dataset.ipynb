{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Target classes: ['class_0' 'class_1' 'class_2']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "\n",
    "# Explore dataset features and target classes\n",
    "print(\"Features:\", wine.feature_names)\n",
    "print(\"Target classes:\", wine.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (124, 13)\n",
      "Shape of X_test: (54, 13)\n",
      "Shape of y_train: (124,)\n",
      "Shape of y_test: (54,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# TODO: Split the dataset into training and testing sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,stratify=y)\n",
    "# Display the shapes of the resulting splits\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of X_train_tensor: tensor([ 1.1106, -0.6648,  0.8724, -0.7386, -0.4595,  0.2207,  1.0223, -1.0826,\n",
      "         1.1635,  0.2843,  1.2907,  1.0697,  1.7044])\n",
      "Sample of y_train_tensor: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# TODO: Convert scaled data to PyTorch tensors\n",
    "# Convert the scaled training and testing data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Display example tensors\n",
    "print(\"Sample of X_train_tensor:\", X_train_tensor[0])\n",
    "print(\"Sample of y_train_tensor:\", y_train_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([124, 13]) torch.Size([54, 13]) torch.Size([124]) torch.Size([54])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define a function load_preprocessed_data that performs the following steps:\n",
    "def load_preprocessed_data():\n",
    "    # 1. Load the Wine dataset\n",
    "    wine = load_wine()\n",
    "    # 2. Split the dataset into training and testing sets\n",
    "    X, y = wine.data, wine.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "    # 3. Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 4. Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    \n",
    "    # 5. Return the processed x and y tensors for training and testing \n",
    "    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor\n",
    "\n",
    "# Call load_preprocessed_data and print the shapes of the returned tensors\n",
    "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = load_preprocessed_data()\n",
    "print(X_train_tensor.shape, X_test_tensor.shape, y_train_tensor.shape, y_test_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/150], Loss: 1.0905, Validation Loss: 1.0957\n",
      "Epoch [20/150], Loss: 1.0566, Validation Loss: 1.0657\n",
      "Epoch [30/150], Loss: 1.0179, Validation Loss: 1.0306\n",
      "Epoch [40/150], Loss: 0.9722, Validation Loss: 0.9892\n",
      "Epoch [50/150], Loss: 0.9186, Validation Loss: 0.9402\n",
      "Epoch [60/150], Loss: 0.8578, Validation Loss: 0.8855\n",
      "Epoch [70/150], Loss: 0.7929, Validation Loss: 0.8268\n",
      "Epoch [80/150], Loss: 0.7280, Validation Loss: 0.7667\n",
      "Epoch [90/150], Loss: 0.6656, Validation Loss: 0.7083\n",
      "Epoch [100/150], Loss: 0.6065, Validation Loss: 0.6535\n",
      "Epoch [110/150], Loss: 0.5530, Validation Loss: 0.6050\n",
      "Epoch [120/150], Loss: 0.5066, Validation Loss: 0.5628\n",
      "Epoch [130/150], Loss: 0.4668, Validation Loss: 0.5252\n",
      "Epoch [140/150], Loss: 0.4326, Validation Loss: 0.4924\n",
      "Epoch [150/150], Loss: 0.4023, Validation Loss: 0.4624\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    # 1. Load the Wine dataset\n",
    "    wine = load_wine()\n",
    "    # 2. Split the dataset into training and testing sets\n",
    "    X, y = wine.data, wine.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "    # 3. Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 4. Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    \n",
    "    # 5. Return the processed x and y tensors for training and testing \n",
    "    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3)\n",
    ")\n",
    "\n",
    "# Define criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 150\n",
    "history = {'loss': [], 'val_loss': []}\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    history['loss'].append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_val = model(X_test)\n",
    "        val_loss = criterion(outputs_val, y_test)\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 20, loss:0.6198025941848755\n",
      "Epoch : 40, loss:0.31639376282691956\n",
      "Epoch : 60, loss:0.21563369035720825\n",
      "Epoch : 80, loss:0.1601736694574356\n",
      "Epoch : 100, loss:0.12346648424863815\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    # 1. Load the Wine dataset\n",
    "    wine = load_wine()\n",
    "    # 2. Split the dataset into training and testing sets\n",
    "    X, y = wine.data, wine.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "    # 3. Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 4. Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    \n",
    "    # 5. Return the processed x and y tensors for training and testing \n",
    "    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "    \n",
    "    \n",
    "simplenet = nn.Sequential(\n",
    "            nn.Linear(13, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10,10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10,3),\n",
    "        )\n",
    "\n",
    "critierion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(simplenet.parameters(),lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    simplenet.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = simplenet(X_train)\n",
    "    loss = critierion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch +1)%20 == 0:\n",
    "        print(f\"Epoch : {epoch+1}, loss:{loss.item()}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 0.7791, Validation Loss: 0.7103\n",
      "Epoch [20/200], Loss: 0.3135, Validation Loss: 0.2618\n",
      "Epoch [30/200], Loss: 0.0792, Validation Loss: 0.1112\n",
      "Epoch [40/200], Loss: 0.0219, Validation Loss: 0.1092\n",
      "Epoch [50/200], Loss: 0.0081, Validation Loss: 0.1029\n",
      "Epoch [60/200], Loss: 0.0043, Validation Loss: 0.1215\n",
      "Epoch [70/200], Loss: 0.0029, Validation Loss: 0.1445\n",
      "Epoch [80/200], Loss: 0.0022, Validation Loss: 0.1549\n",
      "Epoch [90/200], Loss: 0.0018, Validation Loss: 0.1584\n",
      "Epoch [100/200], Loss: 0.0015, Validation Loss: 0.1617\n",
      "Epoch [110/200], Loss: 0.0013, Validation Loss: 0.1655\n",
      "Epoch [120/200], Loss: 0.0012, Validation Loss: 0.1686\n",
      "Epoch [130/200], Loss: 0.0011, Validation Loss: 0.1713\n",
      "Epoch [140/200], Loss: 0.0009, Validation Loss: 0.1737\n",
      "Epoch [150/200], Loss: 0.0009, Validation Loss: 0.1760\n",
      "Epoch [160/200], Loss: 0.0008, Validation Loss: 0.1782\n",
      "Epoch [170/200], Loss: 0.0007, Validation Loss: 0.1804\n",
      "Epoch [180/200], Loss: 0.0006, Validation Loss: 0.1824\n",
      "Epoch [190/200], Loss: 0.0006, Validation Loss: 0.1842\n",
      "Epoch [200/200], Loss: 0.0006, Validation Loss: 0.1860\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3)\n",
    ")\n",
    "\n",
    "# Define criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 200\n",
    "history = {'loss': [], 'val_loss': []}\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)  \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    history['loss'].append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_val = model(X_test)\n",
    "        val_loss = criterion(outputs_val, y_test)  \n",
    "        history['val_loss'].append(val_loss.item())\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/150], Loss: 1.0576, Validation Loss: 1.0529\n",
      "Epoch [20/150], Loss: 1.0222, Validation Loss: 1.0189\n",
      "Epoch [30/150], Loss: 0.9792, Validation Loss: 0.9764\n",
      "Epoch [40/150], Loss: 0.9222, Validation Loss: 0.9201\n",
      "Epoch [50/150], Loss: 0.8434, Validation Loss: 0.8422\n",
      "Epoch [60/150], Loss: 0.7420, Validation Loss: 0.7434\n",
      "Epoch [70/150], Loss: 0.6235, Validation Loss: 0.6271\n",
      "Epoch [80/150], Loss: 0.4992, Validation Loss: 0.5043\n",
      "Epoch [90/150], Loss: 0.3832, Validation Loss: 0.3903\n",
      "Epoch [100/150], Loss: 0.2876, Validation Loss: 0.2957\n",
      "Epoch [110/150], Loss: 0.2158, Validation Loss: 0.2230\n",
      "Epoch [120/150], Loss: 0.1644, Validation Loss: 0.1703\n",
      "Epoch [130/150], Loss: 0.1287, Validation Loss: 0.1328\n",
      "Epoch [140/150], Loss: 0.1035, Validation Loss: 0.1060\n",
      "Epoch [150/150], Loss: 0.0851, Validation Loss: 0.0864\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13,20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,15),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(15,3)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "# Train the model\n",
    "num_epochs = 150\n",
    "history = {'loss': [], 'val_loss': []}\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    history['loss'].append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_val = model(X_test)\n",
    "        val_loss = criterion(outputs_val, y_test)\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
